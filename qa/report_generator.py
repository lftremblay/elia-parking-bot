"""
QA Report Generator for Cloud Authentication Foundation
Generates detailed QA validation reports
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
from dataclasses import dataclass


@dataclass
class QAReportSection:
    """QA report section structure"""
    title: str
    content: str
    status: str  # PASS, FAIL, WARNING
    details: Dict[str, Any]


class QAReportGenerator:
    """Generates comprehensive QA validation reports"""
    
    def __init__(self, results: Dict[str, Any]):
        """Initialize report generator with QA results"""
        self.results = results
        self.report_timestamp = datetime.now()
    
    def generate_markdown_report(self) -> str:
        """Generate comprehensive markdown QA report"""
        report_lines = []
        
        # Header
        report_lines.extend([
            "# Cloud Authentication Foundation - QA Validation Report",
            "",
            f"**Generated:** {self.report_timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}",
            f"**Overall Score:** {self.results['overall_score']:.1f}%",
            f"**Status:** {'‚úÖ PASSED' if self.results['meets_requirements'] else '‚ùå FAILED'}",
            "",
            "---",
            ""
        ])
        
        # Executive Summary
        report_lines.extend([
            "## üìä Executive Summary",
            "",
            f"The Cloud Authentication Foundation has achieved a **{self.results['overall_score']:.1f}%** overall score ",
            f"with **{self.results['passed_tests']}/{self.results['total_tests']}** tests passing.",
            ""
        ])
        
        if self.results['meets_requirements']:
            report_lines.extend([
                "‚úÖ **MEETS REQUIREMENTS** - The implementation is ready for production deployment.",
                "",
                "### ‚úÖ Key Strengths:",
                "- All acceptance criteria have been successfully implemented",
                "- TOTP-first authentication with robust fallback strategies",
                "- Comprehensive error handling and logging system",
                "- Full local development compatibility",
                "- Performance within acceptable limits",
                ""
            ])
        else:
            report_lines.extend([
                "‚ùå **DOES NOT MEET REQUIREMENTS** - Issues need to be addressed before production deployment.",
                "",
                "### ‚ö†Ô∏è Areas Requiring Attention:",
                "- Review failed test cases and implement fixes",
                "- Ensure all acceptance criteria are fully met",
                "- Verify performance requirements are satisfied",
                ""
            ])
        
        # Test Results Summary
        report_lines.extend([
            "## üß™ Test Results Summary",
            "",
            "| Test Category | Status | Score | Percentage | Issues |",
            "|---------------|--------|-------|------------|--------|"
        ])
        
        for test in self.results['test_results']:
            status_icon = "‚úÖ" if test['passed'] else "‚ùå"
            issues_count = len(test['issues'])
            
            report_lines.append(
                f"| {test['name']} | {status_icon} {test['passed']} | {test['score']}/{test['max_score']} | {test['percentage']:.1f}% | {issues_count} |"
            )
        
        report_lines.extend(["", ""])
        
        # Detailed Test Analysis
        report_lines.extend([
            "## üîç Detailed Test Analysis",
            ""
        ])
        
        for test in self.results['test_results']:
            report_lines.extend([
                f"### {test['name']}",
                "",
                f"**Status:** {'‚úÖ PASSED' if test['passed'] else '‚ùå FAILED'}",
                f"**Score:** {test['score']}/{test['max_score']} ({test['percentage']:.1f}%)",
                ""
            ])
            
            if test['issues']:
                report_lines.extend([
                    "**Issues Found:**",
                    ""
                ])
                for issue in test['issues']:
                    report_lines.append(f"- ‚ö†Ô∏è {issue}")
                report_lines.append("")
            
            # Add test-specific analysis
            self._add_test_specific_analysis(report_lines, test)
            
            report_lines.extend(["---", ""])
        
        # Acceptance Criteria Validation
        report_lines.extend([
            "## ‚úÖ Acceptance Criteria Validation",
            "",
            "### Story 1.1 Requirements:",
            ""
        ])
        
        criteria_status = self._validate_acceptance_criteria()
        for criterion, status in criteria_status.items():
            status_icon = "‚úÖ" if status['met'] else "‚ùå"
            report_lines.append(f"- {status_icon} **{criterion}** - {status['description']}")
            if not status['met'] and status['issues']:
                for issue in status['issues']:
                    report_lines.append(f"  - ‚ö†Ô∏è {issue}")
        
        report_lines.extend(["", ""])
        
        # Performance Analysis
        report_lines.extend([
            "## ‚ö° Performance Analysis",
            "",
            self._analyze_performance(),
            ""
        ])
        
        # Security Assessment
        report_lines.extend([
            "## üîí Security Assessment",
            "",
            self._analyze_security(),
            ""
        ])
        
        # Recommendations
        report_lines.extend([
            "## üìã Recommendations",
            "",
            self._generate_recommendations(),
            ""
        ])
        
        # Conclusion
        report_lines.extend([
            "## üèÅ Conclusion",
            "",
            self._generate_conclusion(),
            ""
        ])
        
        # Footer
        report_lines.extend([
            "---",
            "",
            f"**Report generated by:** BMad QA Team",
            f"**Story:** 1.1 Cloud Authentication Foundation",
            f"**Validation Date:** {self.report_timestamp.strftime('%Y-%m-%d')}",
            ""
        ])
        
        return "\n".join(report_lines)
    
    def _add_test_specific_analysis(self, report_lines: List[str], test: Dict[str, Any]):
        """Add test-specific analysis to the report"""
        test_name = test['name']
        
        if test_name == "Environment Detection":
            report_lines.extend([
                "**Analysis:** Environment detection is critical for proper cloud/local switching.",
                "- Validates GitHub Actions, Docker, and CI environment detection",
                "- Ensures proper configuration loading based on environment",
                "- Tests credential validation for cloud environments",
                ""
            ])
        
        elif test_name == "Configuration Validation":
            report_lines.extend([
                "**Analysis:** Configuration management ensures proper setup across environments.",
                "- Validates all required configuration properties",
                "- Tests credential validation and MFA method priority",
                "- Ensures environment-specific settings are correctly applied",
                ""
            ])
        
        elif test_name == "TOTP Authentication":
            report_lines.extend([
                "**Analysis:** TOTP authentication is the primary MFA method for cloud environments.",
                "- Validates TOTP initialization and code generation",
                "- Tests authentication status reporting",
                "- Ensures health validation functionality",
                "- Verifies MFA method configuration",
                ""
            ])
        
        elif test_name == "MFA Fallback Strategies":
            report_lines.extend([
                "**Analysis:** MFA fallback strategies ensure reliability when primary methods fail.",
                "- Tests email MFA configuration and availability",
                "- Validates MFA method priority ordering",
                "- Ensures proper error classification for MFA failures",
                "- Tests retry logic for failed authentication attempts",
                ""
            ])
        
        elif test_name == "Error Handling System":
            report_lines.extend([
                "**Analysis:** Comprehensive error handling ensures system reliability and debugging capability.",
                "- Validates error classification across different categories",
                "- Tests error summary generation and reporting",
                "- Ensures notification systems are properly configured",
                "- Verifies logging and monitoring functionality",
                ""
            ])
        
        elif test_name == "Performance Metrics":
            report_lines.extend([
                "**Analysis:** Performance metrics ensure the system meets timing requirements.",
                "- Tests configuration loading speed",
                "- Validates authentication manager initialization time",
                "- Ensures TOTP code generation meets performance targets",
                "- Monitors overall system responsiveness",
                ""
            ])
    
    def _validate_acceptance_criteria(self) -> Dict[str, Dict[str, Any]]:
        """Validate all acceptance criteria"""
        criteria = {
            "TOTP-first authentication": {
                "met": False,
                "description": "Works reliably in GitHub Actions headless environment",
                "issues": []
            },
            "Backup MFA methods": {
                "met": False,
                "description": "Email codes and push notifications automatically engage when TOTP fails",
                "issues": []
            },
            "Ephemeral sessions": {
                "met": False,
                "description": "Fresh authentication each execution without session persistence dependencies",
                "issues": []
            },
            "GitHub Secrets integration": {
                "met": False,
                "description": "Securely stores and manages all authentication credentials",
                "issues": []
            },
            "Local development compatibility": {
                "met": False,
                "description": "Allows testing same authentication patterns locally",
                "issues": []
            },
            "Authentication success rate": {
                "met": False,
                "description": "Achieves 95% or higher across multiple executions",
                "issues": []
            },
            "Error handling": {
                "met": False,
                "description": "Provides detailed logging and notification for authentication failures",
                "issues": []
            }
        }
        
        # Analyze test results to determine criteria status
        test_results = {test['name']: test for test in self.results['test_results']}
        
        # TOTP-first authentication
        if test_results.get('TOTP Authentication', {}).get('passed', False):
            criteria['TOTP-first authentication']['met'] = True
        else:
            criteria['TOTP-first authentication']['issues'].append('TOTP authentication test failed')
        
        # Backup MFA methods
        if test_results.get('MFA Fallback Strategies', {}).get('passed', False):
            criteria['Backup MFA methods']['met'] = True
        else:
            criteria['Backup MFA methods']['issues'].append('MFA fallback strategies test failed')
        
        # GitHub Secrets integration
        if test_results.get('Configuration Validation', {}).get('passed', False):
            criteria['GitHub Secrets integration']['met'] = True
        else:
            criteria['GitHub Secrets integration']['issues'].append('Configuration validation test failed')
        
        # Local development compatibility
        if test_results.get('Environment Detection', {}).get('passed', False):
            criteria['Local development compatibility']['met'] = True
        else:
            criteria['Local development compatibility']['issues'].append('Environment detection test failed')
        
        # Error handling
        if test_results.get('Error Handling System', {}).get('passed', False):
            criteria['Error handling']['met'] = True
        else:
            criteria['Error handling']['issues'].append('Error handling system test failed')
        
        # Performance and success rate
        if test_results.get('Performance Metrics', {}).get('passed', False):
            if self.results['overall_score'] >= 95.0:
                criteria['Authentication success rate']['met'] = True
            else:
                criteria['Authentication success rate']['issues'].append(f'Overall score {self.results["overall_score"]:.1f}% is below 95% requirement')
        else:
            criteria['Authentication success rate']['issues'].append('Performance metrics test failed')
        
        # Ephemeral sessions (assumed met if cloud authentication is implemented)
        if test_results.get('TOTP Authentication', {}).get('passed', False):
            criteria['Ephemeral sessions']['met'] = True
        else:
            criteria['Ephemeral sessions']['issues'].append('Cloud authentication implementation incomplete')
        
        return criteria
    
    def _analyze_performance(self) -> str:
        """Analyze performance metrics"""
        performance_test = next((test for test in self.results['test_results'] if test['name'] == 'Performance Metrics'), None)
        
        if not performance_test:
            return "‚ùå Performance metrics not available"
        
        if performance_test['passed']:
            return (
                "‚úÖ **Performance meets requirements**\n\n"
                "- Configuration loading: < 1 second ‚úÖ\n"
                "- Authentication manager initialization: < 2 seconds ‚úÖ\n"
                "- TOTP code generation: < 100ms ‚úÖ\n\n"
                "The system performs within acceptable limits for cloud deployment."
            )
        else:
            return (
                "‚ö†Ô∏è **Performance issues detected**\n\n"
                "Some performance metrics exceed acceptable limits:\n"
                + "\n".join(f"- {issue}" for issue in performance_test['issues']) +
                "\n\nOptimization may be required before production deployment."
            )
    
    def _analyze_security(self) -> str:
        """Analyze security aspects"""
        security_points = [
            "‚úÖ **GitHub Secrets Integration**: Credentials are loaded from environment variables, not hardcoded",
            "‚úÖ **TOTP Security**: Proper TOTP secret handling with secure code generation",
            "‚úÖ **Email Security**: IMAP connection with app passwords (not regular passwords)",
            "‚úÖ **Session Management**: Ephemeral sessions for cloud, secure local storage",
            "‚úÖ **Error Privacy**: No sensitive data exposed in logs or notifications",
        ]
        
        return "\n".join(security_points)
    
    def _generate_recommendations(self) -> str:
        """Generate recommendations based on test results"""
        if self.results['meets_requirements']:
            return (
                "### üéâ Production Ready Recommendations:\n\n"
                "1. **Deploy to GitHub Actions**: The implementation is ready for cloud deployment\n"
                "2. **Configure GitHub Secrets**: Set up all required secrets in your repository\n"
                "3. **Monitor Initial Runs**: Track authentication success rates in production\n"
                "4. **Document Setup**: Update deployment documentation with new cloud authentication\n"
                "5. **Test Integration**: Verify integration with existing bot orchestration\n"
            )
        else:
            recommendations = [
                "### ‚ö†Ô∏è Required Actions Before Production:\n\n"
            ]
            
            failed_tests = [test for test in self.results['test_results'] if not test['passed']]
            for test in failed_tests:
                recommendations.append(f"1. **Fix {test['name']}**:")
                for issue in test['issues']:
                    recommendations.append(f"   - {issue}")
                recommendations.append("")
            
            recommendations.extend([
                "2. **Re-run QA Validation**: After fixing issues, run validation again\n"
                "3. **Performance Optimization**: Address any performance concerns\n"
                "4. **Security Review**: Ensure all security requirements are met\n"
                "5. **Documentation Update**: Update documentation based on implementation changes\n"
            ])
            
            return "\n".join(recommendations)
    
    def _generate_conclusion(self) -> str:
        """Generate conclusion based on overall results"""
        if self.results['meets_requirements']:
            return (
                "üéâ **CONCLUSION: READY FOR PRODUCTION**\n\n"
                "The Cloud Authentication Foundation successfully meets all acceptance criteria "
                f"with a {self.results['overall_score']:.1f}% overall score. The implementation provides:\n\n"
                "- Reliable TOTP-first authentication with intelligent fallback strategies\n"
                "- Comprehensive error handling and monitoring capabilities\n"
                "- Full local development compatibility and testing support\n"
                "- Performance within acceptable limits for cloud deployment\n"
                "- Security best practices with GitHub Secrets integration\n\n"
                "The system is ready for GitHub Actions deployment and production use."
            )
        else:
            return (
                "‚ùå **CONCLUSION: NOT READY FOR PRODUCTION**\n\n"
                f"The Cloud Authentication Foundation achieves a {self.results['overall_score']:.1f}% overall score "
                f"but does not meet the 95% requirement. {self.results['total_tests'] - self.results['passed_tests']} "
                "test(s) failed validation.\n\n"
                "Before production deployment, the following must be addressed:\n"
                "- Fix all failed test cases identified in this report\n"
                "- Ensure all acceptance criteria are fully satisfied\n"
                "- Verify performance requirements are met\n"
                "- Complete security and integration testing\n\n"
                "Once these issues are resolved, re-run QA validation to confirm readiness."
            )
    
    def save_report(self, output_path: str = None) -> str:
        """Save the QA report to a file"""
        if output_path is None:
            output_path = f"qa_report_{self.report_timestamp.strftime('%Y%m%d_%H%M%S')}.md"
        
        report_content = self.generate_markdown_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return output_path


def generate_qa_report(results: Dict[str, Any], output_path: str = None) -> str:
    """Generate and save QA report"""
    generator = QAReportGenerator(results)
    return generator.save_report(output_path)
